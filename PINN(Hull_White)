```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from scipy.stats import norm
from scipy.interpolate import UnivariateSpline
from tqdm import tqdm
from torch.utils.data import DataLoader, TensorDataset, random_split
import copy
import random

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

try:
    df_full = pd.read_csv('Yield_curve(2010-2024).csv', index_col='Date', parse_dates=True)
    df_full = df_full.interpolate(method='linear').dropna(axis=1, how='any').sort_index()
except FileNotFoundError:
    print("Error: 'Yield_curve(2010-2024).csv' not found.")
    exit()
    
maturities_np = np.array([float(m.split(' ')[0]) / 12 if 'Mo' in m else float(m.split(' ')[0]) for m in df_full.columns])
n_maturities = len(maturities_np)
SPLINE_SMOOTHING = 2e-3

def B_func_np(t, T, a):
    if abs(a) < 1e-6: return T - t
    return (1 - np.exp(-a * (T - t))) / a

def hull_white_call_option_price(K, t, S, T, r_t, a, sigma, maturities_np, yields_np):
    spline = UnivariateSpline(maturities_np, yields_np, s=SPLINE_SMOOTHING, k=3, ext=1)
    
    def P_np(tau):
        y = spline(tau)
        return np.exp(-y * tau)
        
    P_t_S = P_np(S - t)
    P_t_T = P_np(T - t)
    
    if np.isnan(P_t_S) or np.isnan(P_t_T): return np.nan
    
    if abs(a) > 1e-6:
        sigma_p_term = (sigma**2 / (2 * a)) * (1 - np.exp(-2 * a * (S - t)))
    else:
        sigma_p_term = (sigma**2) * (S - t)
        
    sigma_p = np.sqrt(np.maximum(0, sigma_p_term)) * B_func_np(S, T, a)
    
    if sigma_p < 1e-9: 
        return max(0.0, P_t_T - K * P_t_S)
    
    with np.errstate(all='ignore'):
        d1 = (np.log(P_t_T / (K * P_t_S)) / sigma_p) + 0.5 * sigma_p
        d2 = d1 - sigma_p
        
    if not np.isfinite(d1) or not np.isfinite(d2): return np.nan
    
    return P_t_T * norm.cdf(d1) - K * P_t_S * norm.cdf(d2)

def get_f0t(t, spline):
    t = max(t, 1e-6)
    y_t = spline(t)
    y_prime_t = spline.derivative(1)(t)
    return y_t + t * y_prime_t

def get_theta_t(t, a, sigma, spline):
    t = max(t, 1e-6)
    f_0_t = get_f0t(t, spline)
    
    y_prime_t = spline.derivative(1)(t)
    y_double_prime_t = spline.derivative(2)(t)
    f_prime_0_t = 2 * y_prime_t + t * y_double_prime_t
    
    sigma_term = (sigma**2 / (2 * a)) * (1 - np.exp(-2 * a * t)) if a > 1e-6 else (sigma**2) * t
    
    theta = f_prime_0_t + a * f_0_t + sigma_term
    return np.clip(theta, -0.5, 0.5)

def generate_dataset_from_yields(df_yields, samples_per_day):
    all_features, all_labels = [], []
    for date in tqdm(df_yields.index, desc="Generating PINN Data"):
        yields_np = df_yields.loc[date].values / 100.0
        if np.isnan(yields_np).any(): continue
        
        spline = UnivariateSpline(maturities_np, yields_np, s=SPLINE_SMOOTHING, k=3, ext=1)
        r0_current = spline(1e-6)
        
        for _ in range(samples_per_day):
            a = np.random.uniform(0.05, 0.5)
            sigma = np.random.uniform(0.01, 0.15)
            S = np.random.uniform(0.5, 5.0)
            T = S + np.random.uniform(1.0, 10.0)
            
            try:
                forward_price = np.exp(-spline(T) * T) / np.exp(-spline(S) * S)
            except: continue
            
            if not np.isfinite(forward_price) or forward_price < 1e-9: continue
            
            K = forward_price * np.random.uniform(0.9, 1.1)
            
            option_price = hull_white_call_option_price(K, 0.0, S, T, r0_current, a, sigma, maturities_np, yields_np)
            
            if np.isfinite(option_price) and 1e-7 < option_price < 1.0:
                feature_vector = [S, T, K, a, sigma, r0_current] + list(yields_np)
                all_features.append(torch.tensor(feature_vector, dtype=torch.float32))
                all_labels.append(torch.tensor([option_price], dtype=torch.float32))

    if not all_features: return None
    return TensorDataset(torch.stack(all_features), torch.stack(all_labels))

def get_yield_at_t_torch(t_tensor, maturities_tensor, yields_tensor):
    batch_size = t_tensor.shape[0]
    t_clamped = torch.clamp(t_tensor, min=maturities_tensor[0], max=maturities_tensor[-1])
    idx = torch.searchsorted(maturities_tensor, t_clamped.squeeze())
    idx_lower = torch.clamp(idx - 1, min=0)
    idx_upper = torch.clamp(idx, max=len(maturities_tensor) - 1)
    
    m_lower = maturities_tensor[idx_lower]
    m_upper = maturities_tensor[idx_upper]
    
    y_lower = yields_tensor[torch.arange(batch_size), idx_lower]
    y_upper = yields_tensor[torch.arange(batch_size), idx_upper]
    
    weight = (t_clamped.squeeze() - m_lower) / (m_upper - m_lower + 1e-6)
    weight = torch.clamp(weight, 0.0, 1.0)
    
    y_interpolated = y_lower + weight * (y_upper - y_lower)
    return y_interpolated.unsqueeze(1)

class PINN_HighDim(nn.Module):
    def __init__(self, input_dim, hidden_dim=256, n_layers=6):
        super().__init__()
        layers = [nn.Linear(input_dim, hidden_dim), nn.Tanh()]
        for _ in range(n_layers - 1):
            layers.extend([nn.Linear(hidden_dim, hidden_dim), nn.Tanh()])
        layers.append(nn.Linear(hidden_dim, 1))
        self.network = nn.Sequential(*layers)

    def forward(self, x):
        return F.softplus(self.network(x))

SAMPLES_PER_DAY = 20
full_dataset = generate_dataset_from_yields(df_full, SAMPLES_PER_DAY)
print(f"\nTotal dataset size: {len(full_dataset)}")

dataset_size = len(full_dataset)
train_val_size = int(0.85 * dataset_size)
test_size = dataset_size - train_val_size
train_val_dataset, test_dataset = random_split(full_dataset, [train_val_size, test_size], generator=torch.Generator().manual_seed(42))

val_size = int(0.18 * train_val_size)
train_size = train_val_size - val_size
train_dataset, val_dataset = random_split(train_val_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)
val_loader = DataLoader(val_dataset, batch_size=512)
test_loader = DataLoader(test_dataset, batch_size=512)

base_params = 6 
input_dim_pinn = 2 + base_params + n_maturities
IDX_YIELDS_START = 6
loss_fn = nn.MSELoss()

torch.manual_seed(SEED)
model = PINN_HighDim(input_dim=input_dim_pinn).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=3e-4) 
scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=100, T_mult=2, eta_min=1e-7)

num_epochs = 1000
best_val_loss = float('inf')
best_model_weights = None
patience = 100
patience_counter = 0
lambda_terminal = 10.0 

maturities_tensor = torch.tensor(maturities_np, device=device, dtype=torch.float32)

print(f"Training samples: {len(train_dataset)}")
print(f"Validation samples: {len(val_dataset)}")
print(f"Test samples: {len(test_dataset)}")

for epoch in tqdm(range(num_epochs), desc="Training PINN(Hull-White)"):
    model.train()
    for features_batch_cpu, labels_batch_cpu in train_loader:
        if features_batch_cpu.size(0) == 0: continue
        features_batch = features_batch_cpu.to(device, non_blocking=True)
        
        optimizer.zero_grad()
        
        batch_size = features_batch.size(0)
        S_option = features_batch[:, 0].unsqueeze(1)
        T_option = features_batch[:, 1].unsqueeze(1)
        K_option = features_batch[:, 2].unsqueeze(1)
        a = features_batch[:, 3]
        sigma = features_batch[:, 4]
        r0_current = features_batch[:, 5].unsqueeze(1)
        yields_batch = features_batch[:, IDX_YIELDS_START:]

        t_pde = torch.rand(batch_size, 1, device=device) * S_option
        t_pde.requires_grad = True

        a_vec = a.unsqueeze(1)
        sigma_vec = sigma.unsqueeze(1)

        variance_t = (sigma_vec**2 / (2 * a_vec + 1e-6)) * (1 - torch.exp(-2 * a_vec * t_pde))
        std_t = torch.sqrt(torch.maximum(variance_t, torch.tensor(1e-6, device=device)))
        
        mean_t = get_yield_at_t_torch(t_pde, maturities_tensor, yields_batch)
        
        noise = torch.randn(batch_size, 1, device=device)
        r_pde = mean_t + std_t * noise 
        r_pde = r_pde.detach().requires_grad_(True)

        t_pde_cpu = t_pde.detach().cpu().numpy().flatten()
        yields_cpu = yields_batch.detach().cpu().numpy()
        a_cpu = a.detach().cpu().numpy()
        sigma_cpu = sigma.detach().cpu().numpy()

        theta_list = []
        for i in range(batch_size):
            spline = UnivariateSpline(maturities_np, yields_cpu[i], s=SPLINE_SMOOTHING, k=3)
            theta_val = get_theta_t(t_pde_cpu[i], a_cpu[i], sigma_cpu[i], spline)
            theta_list.append(theta_val)
        
        theta_t = torch.tensor(theta_list, device=device, dtype=torch.float32).unsqueeze(1)

        model_input_pde = torch.cat([t_pde, r_pde, features_batch], dim=1)
        C_pde = model(model_input_pde)
        
        C_grads = torch.autograd.grad(C_pde, [t_pde, r_pde], grad_outputs=torch.ones_like(C_pde), create_graph=True)
        C_t, C_r = C_grads[0], C_grads[1]
        C_rr = torch.autograd.grad(C_r, r_pde, grad_outputs=torch.ones_like(C_r), create_graph=True)[0]
        
        residual = C_t + (theta_t - a_vec * r_pde) * C_r + 0.5 * sigma_vec**2 * C_rr - r_pde * C_pde
        loss_pde = loss_fn(residual, torch.zeros_like(residual))
        
        t_terminal = S_option
        
        variance_S = (sigma_vec**2 / (2 * a_vec + 1e-6)) * (1 - torch.exp(-2 * a_vec * t_terminal))
        std_S = torch.sqrt(torch.maximum(variance_S, torch.tensor(1e-6, device=device)))
        
        mean_S = get_yield_at_t_torch(t_terminal, maturities_tensor, yields_batch)
        noise_S = torch.randn(batch_size, 1, device=device)
        r_terminal = mean_S + std_S * noise_S
        
        model_input_terminal = torch.cat([t_terminal, r_terminal, features_batch], dim=1)
        C_terminal_pred = model(model_input_terminal)
        
        r_terminal_cpu = r_terminal.detach().cpu().numpy().flatten()
        S_cpu = S_option.detach().cpu().numpy().flatten()
        T_cpu = T_option.detach().cpu().numpy().flatten()
        K_cpu = K_option.detach().cpu().numpy().flatten()
        
        payoff_list = []
        for i in range(batch_size):
            spline = UnivariateSpline(maturities_np, yields_cpu[i], s=SPLINE_SMOOTHING, k=3)
            
            B_val = B_func_np(S_cpu[i], T_cpu[i], a_cpu[i])
            P_0_T = np.exp(-spline(T_cpu[i]) * T_cpu[i])
            P_0_S = np.exp(-spline(S_cpu[i]) * S_cpu[i])
            f0_S = get_f0t(S_cpu[i], spline)
            
            sigma_term_sq = (sigma_cpu[i]**2 / (2 * a_cpu[i])) * (1 - np.exp(-2 * a_cpu[i] * S_cpu[i]))
            
            log_A = np.log(P_0_T / P_0_S) + B_val * f0_S - 0.5 * (B_val**2) * sigma_term_sq
            
            bond_price = np.exp(log_A) * np.exp(-B_val * r_terminal_cpu[i])
            payoff_list.append(max(bond_price - K_cpu[i], 0.0))
            
        true_payoff = torch.tensor(payoff_list, device=device, dtype=torch.float32).unsqueeze(1)
        loss_terminal = loss_fn(C_terminal_pred, true_payoff)
        
        loss_total = loss_pde + lambda_terminal * loss_terminal
        
        loss_total.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()

    model.eval()
    current_val_loss = 0.0
    total_samples = 0
    with torch.no_grad():
        for features_batch_cpu, labels_batch_cpu in val_loader:
            features_batch = features_batch_cpu.to(device, non_blocking=True)
            labels_batch = labels_batch_cpu.to(device, non_blocking=True)
            
            t_data = torch.zeros(features_batch.size(0), 1, device=device)
            r_data = features_batch[:, 5].unsqueeze(1) 
            
            model_input_data = torch.cat([t_data, r_data, features_batch], dim=1)
            preds = model(model_input_data)
            
            loss_val = loss_fn(preds, labels_batch)
            current_val_loss += loss_val.item() * features_batch.size(0)
            total_samples += features_batch.size(0)
            
    current_val_loss /= total_samples
    scheduler.step()
    
    if current_val_loss < best_val_loss:
        best_val_loss = current_val_loss
        best_model_weights = copy.deepcopy(model.state_dict())
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

print(f"Training finished. Best validation MSE: {best_val_loss:.8f}")

model.load_state_dict(best_model_weights)
model.eval()

all_preds = []
all_labels = []
with torch.no_grad():
    for features_batch_cpu, labels_batch_cpu in test_loader:
        features_batch = features_batch_cpu.to(device, non_blocking=True)
        labels_batch = labels_batch_cpu.to(device, non_blocking=True)
        
        t_data = torch.zeros(features_batch.size(0), 1, device=device)
        r_data = features_batch[:, 5].unsqueeze(1)
        model_input_data = torch.cat([t_data, r_data, features_batch], dim=1)
        
        preds = model(model_input_data)
        all_preds.append(preds.cpu().numpy())
        all_labels.append(labels_batch.cpu().numpy())

all_preds = np.concatenate(all_preds)
all_labels = np.concatenate(all_labels)
r2 = r2_score(all_labels, all_preds)
mse = mean_squared_error(all_labels, all_preds)

print("="*50)
print(f"FINAL PINN RESULTS (Hull-White)")
print(f"MSE: {mse:.8f}")
print(f"R2: {r2:.4f}")
print("="*50)

